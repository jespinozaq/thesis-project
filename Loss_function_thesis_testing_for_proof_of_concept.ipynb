{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mock up of how model and loss functions can work and be made into modules\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import ast\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# FIX 1: Better user ID parsing that handles the actual data format\n",
        "def safe_parse_user_ids(user_data):\n",
        "    \"\"\"Parse user IDs from the actual Amazon dataset format... this can be changed for other datasets\"\"\"\n",
        "    if pd.isna(user_data):\n",
        "        return []\n",
        "\n",
        "    if isinstance(user_data, list):\n",
        "        return user_data\n",
        "\n",
        "    if isinstance(user_data, str):\n",
        "        # The actual format appears to be comma-separated user IDs\n",
        "        try:\n",
        "            # Split by comma and clean\n",
        "            user_ids = [uid.strip() for uid in user_data.split(',') if uid.strip()]\n",
        "            return user_ids\n",
        "        except:\n",
        "            return [user_data]\n",
        "\n",
        "    return [str(user_data)]\n",
        "\n",
        "def create_implicit_feedback_dataset(df, negative_samples_ratio=4):\n",
        "    \"\"\"Create implicit feedback dataset with proper user parsing... in the future this will just be the dataset adjusted and tuned for implicit feedback. we will have at least 2 versions of each dataset\"\"\"\n",
        "    all_users = set()\n",
        "    all_products = set()\n",
        "\n",
        "    # Extract users and products from the dataset\n",
        "    for idx, row in df.iterrows():\n",
        "        user_list = safe_parse_user_ids(row['user_id'])\n",
        "        product_id = row['product_id']\n",
        "\n",
        "        all_users.update(user_list)\n",
        "        all_products.add(product_id)\n",
        "\n",
        "    all_users = list(all_users)\n",
        "    all_products = list(all_products)\n",
        "\n",
        "    print(f\"Total users: {len(all_users)}\")\n",
        "    print(f\"Total products: {len(all_products)}\")\n",
        "\n",
        "    # Create positive examples\n",
        "    positive_examples = []\n",
        "    for idx, row in df.iterrows():\n",
        "        user_list = safe_parse_user_ids(row['user_id'])\n",
        "        product_id = row['product_id']\n",
        "\n",
        "        for user in user_list:\n",
        "            positive_examples.append((user, product_id, 1))\n",
        "\n",
        "    print(f\"Positive examples: {len(positive_examples)}\")\n",
        "\n",
        "    # Create negative examples\n",
        "    negative_examples = []\n",
        "    user_positive_products = {}\n",
        "\n",
        "    for user, product, _ in positive_examples:\n",
        "        if user not in user_positive_products:\n",
        "            user_positive_products[user] = set()\n",
        "        user_positive_products[user].add(product)\n",
        "\n",
        "    # FIX 2: Better negative sampling\n",
        "    for user in all_users:\n",
        "        positive_set = user_positive_products.get(user, set())\n",
        "        negative_candidates = [p for p in all_products if p not in positive_set]\n",
        "\n",
        "        if negative_candidates and positive_set:\n",
        "            # Sample more strategically\n",
        "            n_negative = min(len(positive_set) * negative_samples_ratio, len(negative_candidates))\n",
        "            if n_negative > 0:\n",
        "                sampled_negatives = np.random.choice(\n",
        "                    negative_candidates,\n",
        "                    size=n_negative,\n",
        "                    replace=False\n",
        "                )\n",
        "                for product in sampled_negatives:\n",
        "                    negative_examples.append((user, product, 0))\n",
        "\n",
        "    print(f\"Negative examples: {len(negative_examples)}\")\n",
        "\n",
        "    # Combine and shuffle\n",
        "    all_examples = positive_examples + negative_examples\n",
        "    np.random.shuffle(all_examples)\n",
        "\n",
        "    return all_examples, all_users, all_products\n",
        "\n",
        "# FIX 3: Improved model from base model with better initialization and regularization\n",
        "class ImprovedRecommender(nn.Module):\n",
        "    def __init__(self, num_users, num_products, embedding_dim=64, bert_dim=128):\n",
        "        super(ImprovedRecommender, self).__init__()\n",
        "\n",
        "        # User and product embeddings with better initialization\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.product_embedding = nn.Embedding(num_products, embedding_dim)\n",
        "\n",
        "        # Initialize embeddings properly\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.product_embedding.weight)\n",
        "\n",
        "        # BERT feature projection\n",
        "        self.bert_projection = nn.Linear(bert_dim, 64)\n",
        "\n",
        "        # Combined features dimension\n",
        "        combined_dim = embedding_dim * 2 + 64\n",
        "\n",
        "        # simple MLP architecture\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_idx, product_idx, bert_features):\n",
        "        user_emb = self.user_embedding(user_idx)\n",
        "        product_emb = self.product_embedding(product_idx)\n",
        "        bert_proj = self.bert_projection(bert_features)\n",
        "\n",
        "        combined = torch.cat([user_emb, product_emb, bert_proj], dim=1)\n",
        "        prediction = self.mlp(combined)\n",
        "\n",
        "        return prediction.squeeze()\n",
        "\n",
        "# FIX 4: Improved BPR loss implementation... currently still playing with this as it is not up to par yet\n",
        "class ImprovedBPRLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedBPRLoss, self).__init__()\n",
        "\n",
        "    def forward(self, positive_scores, negative_scores):\n",
        "        # Add margin and better numerical stability\n",
        "        diff = positive_scores - negative_scores\n",
        "        # Use a margin to prevent trivial solutions\n",
        "        loss = -torch.log(torch.sigmoid(diff) + 1e-10).mean()\n",
        "        return loss\n",
        "\n",
        "# FIX 5: Better training function with proper pair sampling for BPR... still playing around with this but this is just a \"proof of concept\" will work on this over break\n",
        "def improved_train_model(model, train_loader, loss_fn, optimizer, num_epochs=5, loss_name=\"BCE\"):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            user_idx = batch['user_idx'].to(device)\n",
        "            product_idx = batch['product_idx'].to(device)\n",
        "            bert_features = batch['bert_features'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(user_idx, product_idx, bert_features)\n",
        "\n",
        "            if loss_name == \"BPR\":\n",
        "                # FIX: Proper pair sampling for BPR\n",
        "                positive_mask = labels == 1\n",
        "                negative_mask = labels == 0\n",
        "\n",
        "                if positive_mask.sum() > 0 and negative_mask.sum() > 0:\n",
        "                    positive_scores = predictions[positive_mask]\n",
        "                    negative_scores = predictions[negative_mask]\n",
        "\n",
        "                    # Sample pairs more carefully\n",
        "                    min_size = min(len(positive_scores), len(negative_scores))\n",
        "                    if min_size > 0:\n",
        "                        # Randomly sample pairs\n",
        "                        pos_indices = torch.randperm(len(positive_scores))[:min_size]\n",
        "                        neg_indices = torch.randperm(len(negative_scores))[:min_size]\n",
        "\n",
        "                        positive_scores = positive_scores[pos_indices]\n",
        "                        negative_scores = negative_scores[neg_indices]\n",
        "\n",
        "                        loss = loss_fn(positive_scores, negative_scores)\n",
        "                    else:\n",
        "                        continue\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                # BCE loss\n",
        "                loss = loss_fn(predictions, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            # Add gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        if batch_count > 0:\n",
        "            avg_loss = epoch_loss / batch_count\n",
        "            print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}')\n",
        "        else:\n",
        "            print(f'Epoch {epoch+1} completed. No batches processed.')\n",
        "\n",
        "    return model\n",
        "\n",
        "# FIX 6: Create balanced dataset... we won't be creating datasets but again this is just for proof of concept\n",
        "def create_balanced_dataset(implicit_data, max_negative_ratio=2):\n",
        "    \"\"\"Create a more balanced dataset\"\"\"\n",
        "    positive_examples = [ex for ex in implicit_data if ex[2] == 1]\n",
        "    negative_examples = [ex for ex in implicit_data if ex[2] == 0]\n",
        "\n",
        "    print(f\"Original - Positive: {len(positive_examples)}, Negative: {len(negative_examples)}\")\n",
        "\n",
        "    # Balance the dataset\n",
        "    max_negatives = min(len(negative_examples), len(positive_examples) * max_negative_ratio)\n",
        "    balanced_negatives = np.random.choice(\n",
        "        len(negative_examples),\n",
        "        size=max_negatives,\n",
        "        replace=False\n",
        "    )\n",
        "    balanced_negatives = [negative_examples[i] for i in balanced_negatives]\n",
        "\n",
        "    balanced_data = positive_examples + balanced_negatives\n",
        "    np.random.shuffle(balanced_data)\n",
        "\n",
        "    print(f\"Balanced - Positive: {len(positive_examples)}, Negative: {len(balanced_negatives)}\")\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "print(\"=== IMPROVED SETUP ===\")\n",
        "\n",
        "# Create implicit feedback dataset with proper user parsing\n",
        "implicit_data, all_users, all_products = create_implicit_feedback_dataset(df)\n",
        "\n",
        "# FIX: Create balanced dataset\n",
        "balanced_data = create_balanced_dataset(implicit_data, max_negative_ratio=2)\n",
        "\n",
        "# Create mappings\n",
        "user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
        "product_to_idx = {product: idx for idx, product in enumerate(all_products)}\n",
        "\n",
        "print(f\"User mapping size: {len(user_to_idx)}\")\n",
        "print(f\"Product mapping size: {len(product_to_idx)}\")\n",
        "\n",
        "# Prepare product texts\n",
        "print(\"Preparing product texts...\")\n",
        "product_texts = {}\n",
        "for idx, row in df.iterrows():\n",
        "    product_id = row['product_id']\n",
        "    text = f\"{row.get('product_name', '')} {row.get('about_product', '')} {row.get('category', '')}\"\n",
        "    product_texts[product_id] = text\n",
        "\n",
        "# Use smaller BERT model\n",
        "print(\"Loading BERT model...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "bert_model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "bert_model.eval()\n",
        "\n",
        "print(\"Precomputing BERT embeddings...\")\n",
        "bert_embeddings = {}\n",
        "batch_size = 16\n",
        "\n",
        "product_ids = list(product_texts.keys())\n",
        "for i in range(0, len(product_ids), batch_size):\n",
        "    batch_ids = product_ids[i:i+batch_size]\n",
        "    batch_texts = [product_texts[pid] for pid in batch_ids]\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        batch_texts,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**encoding)\n",
        "        batch_embeddings = outputs.pooler_output\n",
        "\n",
        "    for j, product_id in enumerate(batch_ids):\n",
        "        bert_embeddings[product_id] = batch_embeddings[j].numpy()\n",
        "\n",
        "    if (i // batch_size) % 10 == 0:\n",
        "        print(f\"Processed {i + len(batch_ids)}/{len(product_ids)} products\")\n",
        "\n",
        "print(f\"Precomputed embeddings for {len(bert_embeddings)} products\")\n",
        "\n",
        "# Dataset class\n",
        "class FastAmazonDataset(Dataset):\n",
        "    def __init__(self, data, bert_embeddings, user_to_idx, product_to_idx):\n",
        "        self.data = data\n",
        "        self.bert_embeddings = bert_embeddings\n",
        "        self.user_to_idx = user_to_idx\n",
        "        self.product_to_idx = product_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user, product, label = self.data[idx]\n",
        "\n",
        "        user_idx = self.user_to_idx[user]\n",
        "        product_idx = self.product_to_idx[product]\n",
        "        bert_embedding = self.bert_embeddings.get(product, np.zeros(128))\n",
        "\n",
        "        return {\n",
        "            'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
        "            'product_idx': torch.tensor(product_idx, dtype=torch.long),\n",
        "            'bert_features': torch.tensor(bert_embedding, dtype=torch.float),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Split balanced data\n",
        "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FastAmazonDataset(train_data, bert_embeddings, user_to_idx, product_to_idx)\n",
        "test_dataset = FastAmazonDataset(test_data, bert_embeddings, user_to_idx, product_to_idx)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Initialize model\n",
        "num_users = len(user_to_idx)\n",
        "num_products = len(product_to_idx)\n",
        "bert_dim = 128\n",
        "\n",
        "print(f\"Model parameters: {num_users} users, {num_products} products\")\n",
        "\n",
        "# Test improved loss functions\n",
        "loss_functions = {\n",
        "    \"BCE\": nn.BCELoss(),\n",
        "    \"BPR\": ImprovedBPRLoss()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\n=== IMPROVED TRAINING ===\")\n",
        "\n",
        "for loss_name, loss_fn in loss_functions.items():\n",
        "    print(f\"\\n=== Training with {loss_name} Loss ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize model with better hyperparameters\n",
        "    model = ImprovedRecommender(num_users, num_products, embedding_dim=64, bert_dim=bert_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Train with improved function\n",
        "    model = improved_train_model(model, train_loader, loss_fn, optimizer, num_epochs=5, loss_name=loss_name)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"{loss_name} training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    results[loss_name] = {\n",
        "        'model': model,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "\n",
        "# Evaluation function... this part will be more in depth according to what the best evaluation techniques are from the literature review\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            user_idx = batch['user_idx'].to(device)\n",
        "            product_idx = batch['product_idx'].to(device)\n",
        "            bert_features = batch['bert_features'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            predictions = model(user_idx, product_idx, bert_features)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_predictions), np.array(all_labels)\n",
        "\n",
        "print(\"\\n=== IMPROVED EVALUATION ===\")\n",
        "evaluation_results = {}\n",
        "\n",
        "for loss_name, result in results.items():\n",
        "    model = result['model']\n",
        "    predictions, labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    # Calculate comprehensive metrics... we can keep these as baseline while including other evaluation metrics from the literature review\n",
        "    binary_predictions = (predictions > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(labels, binary_predictions)\n",
        "    precision = precision_score(labels, binary_predictions, zero_division=0)\n",
        "    recall = recall_score(labels, binary_predictions, zero_division=0)\n",
        "    f1 = f1_score(labels, binary_predictions, zero_division=0)\n",
        "    auc = roc_auc_score(labels, predictions)\n",
        "\n",
        "    evaluation_results[loss_name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{loss_name} Loss:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    print(f\"  AUC: {auc:.4f}\")\n",
        "\n",
        "\n",
        "# Recommendation function... this will be the \"engine\" behind the LLM model that will generate/provide the recommendations to the LLM model to give back to the user\n",
        "def get_recommendations(model, user_id, bert_embeddings, user_to_idx, product_to_idx, top_k=5):\n",
        "    model.eval()\n",
        "\n",
        "    if user_id not in user_to_idx:\n",
        "        print(f\"User {user_id} not found\")\n",
        "        return []\n",
        "\n",
        "    user_idx = user_to_idx[user_id]\n",
        "    user_tensor = torch.tensor([user_idx], dtype=torch.long).to(device)\n",
        "\n",
        "    scores = []\n",
        "    product_info = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for product_id, bert_embedding in bert_embeddings.items():\n",
        "            if product_id not in product_to_idx:\n",
        "                continue\n",
        "\n",
        "            product_idx = product_to_idx[product_id]\n",
        "            product_tensor = torch.tensor([product_idx], dtype=torch.long).to(device)\n",
        "            bert_tensor = torch.tensor(bert_embedding, dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "            score = model(user_tensor, product_tensor, bert_tensor)\n",
        "            scores.append(score.item())\n",
        "\n",
        "            product_row = df[df['product_id'] == product_id]\n",
        "            product_name = product_row['product_name'].iloc[0] if len(product_row) > 0 else \"Unknown\"\n",
        "            product_info.append((product_id, product_name))\n",
        "\n",
        "    # Get diverse recommendations (not just highest scores)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "    recommendations = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "        product_id, product_name = product_info[idx]\n",
        "        recommendations.append({\n",
        "            'product_id': product_id,\n",
        "            'product_name': product_name,\n",
        "            'score': scores[idx]\n",
        "        })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Test with a single valid user\n",
        "valid_users = [user for user in user_to_idx.keys() if len(user) > 10]  # Filter out malformed user IDs\n",
        "if valid_users:\n",
        "    sample_user = valid_users[0]\n",
        "    print(f\"\\n=== RECOMMENDATIONS FOR USER {sample_user} ===\")\n",
        "\n",
        "    for loss_name, result in results.items():\n",
        "        recommendations = get_recommendations(\n",
        "            result['model'], sample_user, bert_embeddings, user_to_idx, product_to_idx, top_k=3\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{loss_name} Loss Recommendations:\")\n",
        "        for i, rec in enumerate(recommendations, 1):\n",
        "            print(f\"  {i}. {rec['product_name'][:40]}... (Score: {rec['score']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "for loss_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\n{loss_name}:\")\n",
        "    print(f\"  AUC: {metrics['auc']:.4f} | F1: {metrics['f1_score']:.4f} | Accuracy: {metrics['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n=== TRAINING COMPLETED ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn_f69D3eF9z",
        "outputId": "957840e2-d044-487e-b7bf-a9f923c94000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "=== IMPROVED SETUP ===\n",
            "Total users: 9050\n",
            "Total products: 1351\n",
            "Positive examples: 11503\n",
            "Negative examples: 42416\n",
            "Original - Positive: 11503, Negative: 42416\n",
            "Balanced - Positive: 11503, Negative: 23006\n",
            "User mapping size: 9050\n",
            "Product mapping size: 1351\n",
            "Preparing product texts...\n",
            "Loading BERT model...\n",
            "Precomputing BERT embeddings...\n",
            "Processed 16/1351 products\n",
            "Processed 176/1351 products\n",
            "Processed 336/1351 products\n",
            "Processed 496/1351 products\n",
            "Processed 656/1351 products\n",
            "Processed 816/1351 products\n",
            "Processed 976/1351 products\n",
            "Processed 1136/1351 products\n",
            "Processed 1296/1351 products\n",
            "Precomputed embeddings for 1351 products\n",
            "Train batches: 863\n",
            "Test batches: 216\n",
            "Model parameters: 9050 users, 1351 products\n",
            "\n",
            "=== IMPROVED TRAINING ===\n",
            "\n",
            "=== Training with BCE Loss ===\n",
            "Epoch 1, Batch 0/863, Loss: 0.7017\n",
            "Epoch 1, Batch 20/863, Loss: 0.6223\n",
            "Epoch 1, Batch 40/863, Loss: 0.6599\n",
            "Epoch 1, Batch 60/863, Loss: 0.6340\n",
            "Epoch 1, Batch 80/863, Loss: 0.6654\n",
            "Epoch 1, Batch 100/863, Loss: 0.5236\n",
            "Epoch 1, Batch 120/863, Loss: 0.6325\n",
            "Epoch 1, Batch 140/863, Loss: 0.6666\n",
            "Epoch 1, Batch 160/863, Loss: 0.5552\n",
            "Epoch 1, Batch 180/863, Loss: 0.6202\n",
            "Epoch 1, Batch 200/863, Loss: 0.6183\n",
            "Epoch 1, Batch 220/863, Loss: 0.5399\n",
            "Epoch 1, Batch 240/863, Loss: 0.7147\n",
            "Epoch 1, Batch 260/863, Loss: 0.7185\n",
            "Epoch 1, Batch 280/863, Loss: 0.6716\n",
            "Epoch 1, Batch 300/863, Loss: 0.6366\n",
            "Epoch 1, Batch 320/863, Loss: 0.6619\n",
            "Epoch 1, Batch 340/863, Loss: 0.6112\n",
            "Epoch 1, Batch 360/863, Loss: 0.6021\n",
            "Epoch 1, Batch 380/863, Loss: 0.6131\n",
            "Epoch 1, Batch 400/863, Loss: 0.7151\n",
            "Epoch 1, Batch 420/863, Loss: 0.6665\n",
            "Epoch 1, Batch 440/863, Loss: 0.6713\n",
            "Epoch 1, Batch 460/863, Loss: 0.6403\n",
            "Epoch 1, Batch 480/863, Loss: 0.6996\n",
            "Epoch 1, Batch 500/863, Loss: 0.6553\n",
            "Epoch 1, Batch 520/863, Loss: 0.5801\n",
            "Epoch 1, Batch 540/863, Loss: 0.7314\n",
            "Epoch 1, Batch 560/863, Loss: 0.7480\n",
            "Epoch 1, Batch 580/863, Loss: 0.5599\n",
            "Epoch 1, Batch 600/863, Loss: 0.6056\n",
            "Epoch 1, Batch 620/863, Loss: 0.6750\n",
            "Epoch 1, Batch 640/863, Loss: 0.6113\n",
            "Epoch 1, Batch 660/863, Loss: 0.6630\n",
            "Epoch 1, Batch 680/863, Loss: 0.6856\n",
            "Epoch 1, Batch 700/863, Loss: 0.7408\n",
            "Epoch 1, Batch 720/863, Loss: 0.5984\n",
            "Epoch 1, Batch 740/863, Loss: 0.6414\n",
            "Epoch 1, Batch 760/863, Loss: 0.5927\n",
            "Epoch 1, Batch 780/863, Loss: 0.6232\n",
            "Epoch 1, Batch 800/863, Loss: 0.5577\n",
            "Epoch 1, Batch 820/863, Loss: 0.6222\n",
            "Epoch 1, Batch 840/863, Loss: 0.5748\n",
            "Epoch 1, Batch 860/863, Loss: 0.7153\n",
            "Epoch 1 completed. Average Loss: 0.6423\n",
            "Epoch 2, Batch 0/863, Loss: 0.5647\n",
            "Epoch 2, Batch 20/863, Loss: 0.5818\n",
            "Epoch 2, Batch 40/863, Loss: 0.5650\n",
            "Epoch 2, Batch 60/863, Loss: 0.5282\n",
            "Epoch 2, Batch 80/863, Loss: 0.6689\n",
            "Epoch 2, Batch 100/863, Loss: 0.7082\n",
            "Epoch 2, Batch 120/863, Loss: 0.5396\n",
            "Epoch 2, Batch 140/863, Loss: 0.7596\n",
            "Epoch 2, Batch 160/863, Loss: 0.7197\n",
            "Epoch 2, Batch 180/863, Loss: 0.5453\n",
            "Epoch 2, Batch 200/863, Loss: 0.7542\n",
            "Epoch 2, Batch 220/863, Loss: 0.6663\n",
            "Epoch 2, Batch 240/863, Loss: 0.5297\n",
            "Epoch 2, Batch 260/863, Loss: 0.7137\n",
            "Epoch 2, Batch 280/863, Loss: 0.5339\n",
            "Epoch 2, Batch 300/863, Loss: 0.7044\n",
            "Epoch 2, Batch 320/863, Loss: 0.4832\n",
            "Epoch 2, Batch 340/863, Loss: 0.5811\n",
            "Epoch 2, Batch 360/863, Loss: 0.5253\n",
            "Epoch 2, Batch 380/863, Loss: 0.6104\n",
            "Epoch 2, Batch 400/863, Loss: 0.5186\n",
            "Epoch 2, Batch 420/863, Loss: 0.5534\n",
            "Epoch 2, Batch 440/863, Loss: 0.5570\n",
            "Epoch 2, Batch 460/863, Loss: 0.5681\n",
            "Epoch 2, Batch 480/863, Loss: 0.6436\n",
            "Epoch 2, Batch 500/863, Loss: 0.5065\n",
            "Epoch 2, Batch 520/863, Loss: 0.5573\n",
            "Epoch 2, Batch 540/863, Loss: 0.6026\n",
            "Epoch 2, Batch 560/863, Loss: 0.7696\n",
            "Epoch 2, Batch 580/863, Loss: 0.6620\n",
            "Epoch 2, Batch 600/863, Loss: 0.5911\n",
            "Epoch 2, Batch 620/863, Loss: 0.5836\n",
            "Epoch 2, Batch 640/863, Loss: 0.5327\n",
            "Epoch 2, Batch 660/863, Loss: 0.5417\n",
            "Epoch 2, Batch 680/863, Loss: 0.5754\n",
            "Epoch 2, Batch 700/863, Loss: 0.5527\n",
            "Epoch 2, Batch 720/863, Loss: 0.6144\n",
            "Epoch 2, Batch 740/863, Loss: 0.6973\n",
            "Epoch 2, Batch 760/863, Loss: 0.6436\n",
            "Epoch 2, Batch 780/863, Loss: 0.4748\n",
            "Epoch 2, Batch 800/863, Loss: 0.6128\n",
            "Epoch 2, Batch 820/863, Loss: 0.4490\n",
            "Epoch 2, Batch 840/863, Loss: 0.4679\n",
            "Epoch 2, Batch 860/863, Loss: 0.4830\n",
            "Epoch 2 completed. Average Loss: 0.5873\n",
            "Epoch 3, Batch 0/863, Loss: 0.3760\n",
            "Epoch 3, Batch 20/863, Loss: 0.5362\n",
            "Epoch 3, Batch 40/863, Loss: 0.4728\n",
            "Epoch 3, Batch 60/863, Loss: 0.3238\n",
            "Epoch 3, Batch 80/863, Loss: 0.4719\n",
            "Epoch 3, Batch 100/863, Loss: 0.3052\n",
            "Epoch 3, Batch 120/863, Loss: 0.3888\n",
            "Epoch 3, Batch 140/863, Loss: 0.4157\n",
            "Epoch 3, Batch 160/863, Loss: 0.3484\n",
            "Epoch 3, Batch 180/863, Loss: 0.4195\n",
            "Epoch 3, Batch 200/863, Loss: 0.3134\n",
            "Epoch 3, Batch 220/863, Loss: 0.4076\n",
            "Epoch 3, Batch 240/863, Loss: 0.5537\n",
            "Epoch 3, Batch 260/863, Loss: 0.3564\n",
            "Epoch 3, Batch 280/863, Loss: 0.2439\n",
            "Epoch 3, Batch 300/863, Loss: 0.5357\n",
            "Epoch 3, Batch 320/863, Loss: 0.2894\n",
            "Epoch 3, Batch 340/863, Loss: 0.3573\n",
            "Epoch 3, Batch 360/863, Loss: 0.4040\n",
            "Epoch 3, Batch 380/863, Loss: 0.4193\n",
            "Epoch 3, Batch 400/863, Loss: 0.5885\n",
            "Epoch 3, Batch 420/863, Loss: 0.4313\n",
            "Epoch 3, Batch 440/863, Loss: 0.4511\n",
            "Epoch 3, Batch 460/863, Loss: 0.3284\n",
            "Epoch 3, Batch 480/863, Loss: 0.5123\n",
            "Epoch 3, Batch 500/863, Loss: 0.4861\n",
            "Epoch 3, Batch 520/863, Loss: 0.3118\n",
            "Epoch 3, Batch 540/863, Loss: 0.3801\n",
            "Epoch 3, Batch 560/863, Loss: 0.4399\n",
            "Epoch 3, Batch 580/863, Loss: 0.5805\n",
            "Epoch 3, Batch 600/863, Loss: 0.5391\n",
            "Epoch 3, Batch 620/863, Loss: 0.4255\n",
            "Epoch 3, Batch 640/863, Loss: 0.2978\n",
            "Epoch 3, Batch 660/863, Loss: 0.4091\n",
            "Epoch 3, Batch 680/863, Loss: 0.6205\n",
            "Epoch 3, Batch 700/863, Loss: 0.4330\n",
            "Epoch 3, Batch 720/863, Loss: 0.4962\n",
            "Epoch 3, Batch 740/863, Loss: 0.3320\n",
            "Epoch 3, Batch 760/863, Loss: 0.4976\n",
            "Epoch 3, Batch 780/863, Loss: 0.5116\n",
            "Epoch 3, Batch 800/863, Loss: 0.7040\n",
            "Epoch 3, Batch 820/863, Loss: 0.4137\n",
            "Epoch 3, Batch 840/863, Loss: 0.4337\n",
            "Epoch 3, Batch 860/863, Loss: 0.3732\n",
            "Epoch 3 completed. Average Loss: 0.4459\n",
            "Epoch 4, Batch 0/863, Loss: 0.3605\n",
            "Epoch 4, Batch 20/863, Loss: 0.3593\n",
            "Epoch 4, Batch 40/863, Loss: 0.2387\n",
            "Epoch 4, Batch 60/863, Loss: 0.1683\n",
            "Epoch 4, Batch 80/863, Loss: 0.2757\n",
            "Epoch 4, Batch 100/863, Loss: 0.2580\n",
            "Epoch 4, Batch 120/863, Loss: 0.1936\n",
            "Epoch 4, Batch 140/863, Loss: 0.2248\n",
            "Epoch 4, Batch 160/863, Loss: 0.2796\n",
            "Epoch 4, Batch 180/863, Loss: 0.2613\n",
            "Epoch 4, Batch 200/863, Loss: 0.2698\n",
            "Epoch 4, Batch 220/863, Loss: 0.6113\n",
            "Epoch 4, Batch 240/863, Loss: 0.2882\n",
            "Epoch 4, Batch 260/863, Loss: 0.5466\n",
            "Epoch 4, Batch 280/863, Loss: 0.2644\n",
            "Epoch 4, Batch 300/863, Loss: 0.1563\n",
            "Epoch 4, Batch 320/863, Loss: 0.2344\n",
            "Epoch 4, Batch 340/863, Loss: 0.3255\n",
            "Epoch 4, Batch 360/863, Loss: 0.5287\n",
            "Epoch 4, Batch 380/863, Loss: 0.3023\n",
            "Epoch 4, Batch 400/863, Loss: 0.5186\n",
            "Epoch 4, Batch 420/863, Loss: 0.1888\n",
            "Epoch 4, Batch 440/863, Loss: 0.2160\n",
            "Epoch 4, Batch 460/863, Loss: 0.2230\n",
            "Epoch 4, Batch 480/863, Loss: 0.3490\n",
            "Epoch 4, Batch 500/863, Loss: 0.1298\n",
            "Epoch 4, Batch 520/863, Loss: 0.3146\n",
            "Epoch 4, Batch 540/863, Loss: 0.4820\n",
            "Epoch 4, Batch 560/863, Loss: 0.3094\n",
            "Epoch 4, Batch 580/863, Loss: 0.3635\n",
            "Epoch 4, Batch 600/863, Loss: 0.5222\n",
            "Epoch 4, Batch 620/863, Loss: 0.3309\n",
            "Epoch 4, Batch 640/863, Loss: 0.3817\n",
            "Epoch 4, Batch 660/863, Loss: 0.2204\n",
            "Epoch 4, Batch 680/863, Loss: 0.3059\n",
            "Epoch 4, Batch 700/863, Loss: 0.1687\n",
            "Epoch 4, Batch 720/863, Loss: 0.3838\n",
            "Epoch 4, Batch 740/863, Loss: 0.3619\n",
            "Epoch 4, Batch 760/863, Loss: 0.2922\n",
            "Epoch 4, Batch 780/863, Loss: 0.7466\n",
            "Epoch 4, Batch 800/863, Loss: 0.3523\n",
            "Epoch 4, Batch 820/863, Loss: 0.2821\n",
            "Epoch 4, Batch 840/863, Loss: 0.2838\n",
            "Epoch 4, Batch 860/863, Loss: 0.4338\n",
            "Epoch 4 completed. Average Loss: 0.3255\n",
            "Epoch 5, Batch 0/863, Loss: 0.2177\n",
            "Epoch 5, Batch 20/863, Loss: 0.2013\n",
            "Epoch 5, Batch 40/863, Loss: 0.2338\n",
            "Epoch 5, Batch 60/863, Loss: 0.1492\n",
            "Epoch 5, Batch 80/863, Loss: 0.1359\n",
            "Epoch 5, Batch 100/863, Loss: 0.1703\n",
            "Epoch 5, Batch 120/863, Loss: 0.2444\n",
            "Epoch 5, Batch 140/863, Loss: 0.2506\n",
            "Epoch 5, Batch 160/863, Loss: 0.1415\n",
            "Epoch 5, Batch 180/863, Loss: 0.5481\n",
            "Epoch 5, Batch 200/863, Loss: 0.3807\n",
            "Epoch 5, Batch 220/863, Loss: 0.1397\n",
            "Epoch 5, Batch 240/863, Loss: 0.1513\n",
            "Epoch 5, Batch 260/863, Loss: 0.2662\n",
            "Epoch 5, Batch 280/863, Loss: 0.3410\n",
            "Epoch 5, Batch 300/863, Loss: 0.2666\n",
            "Epoch 5, Batch 320/863, Loss: 0.2342\n",
            "Epoch 5, Batch 340/863, Loss: 0.1757\n",
            "Epoch 5, Batch 360/863, Loss: 0.1907\n",
            "Epoch 5, Batch 380/863, Loss: 0.2461\n",
            "Epoch 5, Batch 400/863, Loss: 0.5002\n",
            "Epoch 5, Batch 420/863, Loss: 0.2155\n",
            "Epoch 5, Batch 440/863, Loss: 0.4400\n",
            "Epoch 5, Batch 460/863, Loss: 0.2151\n",
            "Epoch 5, Batch 480/863, Loss: 0.1928\n",
            "Epoch 5, Batch 500/863, Loss: 0.3081\n",
            "Epoch 5, Batch 520/863, Loss: 0.3654\n",
            "Epoch 5, Batch 540/863, Loss: 0.4820\n",
            "Epoch 5, Batch 560/863, Loss: 0.2537\n",
            "Epoch 5, Batch 580/863, Loss: 0.2706\n",
            "Epoch 5, Batch 600/863, Loss: 0.2386\n",
            "Epoch 5, Batch 620/863, Loss: 0.2935\n",
            "Epoch 5, Batch 640/863, Loss: 0.0962\n",
            "Epoch 5, Batch 660/863, Loss: 0.1729\n",
            "Epoch 5, Batch 680/863, Loss: 0.5485\n",
            "Epoch 5, Batch 700/863, Loss: 0.5025\n",
            "Epoch 5, Batch 720/863, Loss: 0.3044\n",
            "Epoch 5, Batch 740/863, Loss: 0.2005\n",
            "Epoch 5, Batch 760/863, Loss: 0.1511\n",
            "Epoch 5, Batch 780/863, Loss: 0.2341\n",
            "Epoch 5, Batch 800/863, Loss: 0.3780\n",
            "Epoch 5, Batch 820/863, Loss: 0.4920\n",
            "Epoch 5, Batch 840/863, Loss: 0.1689\n",
            "Epoch 5, Batch 860/863, Loss: 0.2953\n",
            "Epoch 5 completed. Average Loss: 0.2430\n",
            "BCE training completed in 50.69 seconds\n",
            "\n",
            "=== Training with BPR Loss ===\n",
            "Epoch 1, Batch 0/863, Loss: 0.6965\n",
            "Epoch 1, Batch 20/863, Loss: 0.7079\n",
            "Epoch 1, Batch 40/863, Loss: 0.6705\n",
            "Epoch 1, Batch 60/863, Loss: 0.6549\n",
            "Epoch 1, Batch 80/863, Loss: 0.7011\n",
            "Epoch 1, Batch 100/863, Loss: 0.7489\n",
            "Epoch 1, Batch 120/863, Loss: 0.6867\n",
            "Epoch 1, Batch 140/863, Loss: 0.6697\n",
            "Epoch 1, Batch 160/863, Loss: 0.7132\n",
            "Epoch 1, Batch 180/863, Loss: 0.6846\n",
            "Epoch 1, Batch 200/863, Loss: 0.6846\n",
            "Epoch 1, Batch 220/863, Loss: 0.6788\n",
            "Epoch 1, Batch 240/863, Loss: 0.6970\n",
            "Epoch 1, Batch 260/863, Loss: 0.6730\n",
            "Epoch 1, Batch 280/863, Loss: 0.6861\n",
            "Epoch 1, Batch 300/863, Loss: 0.6867\n",
            "Epoch 1, Batch 320/863, Loss: 0.6870\n",
            "Epoch 1, Batch 340/863, Loss: 0.7021\n",
            "Epoch 1, Batch 360/863, Loss: 0.7138\n",
            "Epoch 1, Batch 380/863, Loss: 0.6732\n",
            "Epoch 1, Batch 400/863, Loss: 0.6660\n",
            "Epoch 1, Batch 420/863, Loss: 0.6850\n",
            "Epoch 1, Batch 440/863, Loss: 0.6881\n",
            "Epoch 1, Batch 460/863, Loss: 0.7180\n",
            "Epoch 1, Batch 480/863, Loss: 0.7768\n",
            "Epoch 1, Batch 500/863, Loss: 0.6728\n",
            "Epoch 1, Batch 520/863, Loss: 0.6654\n",
            "Epoch 1, Batch 540/863, Loss: 0.6897\n",
            "Epoch 1, Batch 560/863, Loss: 0.6977\n",
            "Epoch 1, Batch 580/863, Loss: 0.6708\n",
            "Epoch 1, Batch 600/863, Loss: 0.7373\n",
            "Epoch 1, Batch 620/863, Loss: 0.7037\n",
            "Epoch 1, Batch 640/863, Loss: 0.7079\n",
            "Epoch 1, Batch 660/863, Loss: 0.6777\n",
            "Epoch 1, Batch 680/863, Loss: 0.6927\n",
            "Epoch 1, Batch 700/863, Loss: 0.7186\n",
            "Epoch 1, Batch 720/863, Loss: 0.6840\n",
            "Epoch 1, Batch 740/863, Loss: 0.6836\n",
            "Epoch 1, Batch 760/863, Loss: 0.6971\n",
            "Epoch 1, Batch 780/863, Loss: 0.7014\n",
            "Epoch 1, Batch 800/863, Loss: 0.6577\n",
            "Epoch 1, Batch 820/863, Loss: 0.7044\n",
            "Epoch 1, Batch 840/863, Loss: 0.6987\n",
            "Epoch 1, Batch 860/863, Loss: 0.6605\n",
            "Epoch 1 completed. Average Loss: 0.6927\n",
            "Epoch 2, Batch 0/863, Loss: 0.6832\n",
            "Epoch 2, Batch 20/863, Loss: 0.6670\n",
            "Epoch 2, Batch 40/863, Loss: 0.6912\n",
            "Epoch 2, Batch 60/863, Loss: 0.6292\n",
            "Epoch 2, Batch 80/863, Loss: 0.6966\n",
            "Epoch 2, Batch 100/863, Loss: 0.6735\n",
            "Epoch 2, Batch 120/863, Loss: 0.6466\n",
            "Epoch 2, Batch 140/863, Loss: 0.7351\n",
            "Epoch 2, Batch 160/863, Loss: 0.6117\n",
            "Epoch 2, Batch 180/863, Loss: 0.6755\n",
            "Epoch 2, Batch 200/863, Loss: 0.5721\n",
            "Epoch 2, Batch 220/863, Loss: 0.6337\n",
            "Epoch 2, Batch 240/863, Loss: 0.6948\n",
            "Epoch 2, Batch 260/863, Loss: 0.6706\n",
            "Epoch 2, Batch 280/863, Loss: 0.6810\n",
            "Epoch 2, Batch 300/863, Loss: 0.6256\n",
            "Epoch 2, Batch 320/863, Loss: 0.6704\n",
            "Epoch 2, Batch 340/863, Loss: 0.5065\n",
            "Epoch 2, Batch 360/863, Loss: 0.6381\n",
            "Epoch 2, Batch 380/863, Loss: 0.7254\n",
            "Epoch 2, Batch 400/863, Loss: 0.5660\n",
            "Epoch 2, Batch 420/863, Loss: 0.7063\n",
            "Epoch 2, Batch 440/863, Loss: 0.6885\n",
            "Epoch 2, Batch 460/863, Loss: 0.7096\n",
            "Epoch 2, Batch 480/863, Loss: 0.6449\n",
            "Epoch 2, Batch 500/863, Loss: 0.6898\n",
            "Epoch 2, Batch 520/863, Loss: 0.7029\n",
            "Epoch 2, Batch 540/863, Loss: 0.5402\n",
            "Epoch 2, Batch 560/863, Loss: 0.5734\n",
            "Epoch 2, Batch 580/863, Loss: 0.7412\n",
            "Epoch 2, Batch 600/863, Loss: 0.6181\n",
            "Epoch 2, Batch 620/863, Loss: 0.5150\n",
            "Epoch 2, Batch 640/863, Loss: 0.5943\n",
            "Epoch 2, Batch 660/863, Loss: 0.6817\n",
            "Epoch 2, Batch 680/863, Loss: 0.6315\n",
            "Epoch 2, Batch 700/863, Loss: 0.5820\n",
            "Epoch 2, Batch 720/863, Loss: 0.5670\n",
            "Epoch 2, Batch 740/863, Loss: 0.7234\n",
            "Epoch 2, Batch 760/863, Loss: 0.7439\n",
            "Epoch 2, Batch 780/863, Loss: 0.7282\n",
            "Epoch 2, Batch 800/863, Loss: 0.7478\n",
            "Epoch 2, Batch 820/863, Loss: 0.6927\n",
            "Epoch 2, Batch 840/863, Loss: 0.6619\n",
            "Epoch 2, Batch 860/863, Loss: 0.7647\n",
            "Epoch 2 completed. Average Loss: 0.6560\n",
            "Epoch 3, Batch 0/863, Loss: 0.6699\n",
            "Epoch 3, Batch 20/863, Loss: 0.5572\n",
            "Epoch 3, Batch 40/863, Loss: 0.5294\n",
            "Epoch 3, Batch 60/863, Loss: 0.5186\n",
            "Epoch 3, Batch 80/863, Loss: 0.5601\n",
            "Epoch 3, Batch 100/863, Loss: 0.4965\n",
            "Epoch 3, Batch 120/863, Loss: 0.5984\n",
            "Epoch 3, Batch 140/863, Loss: 0.4793\n",
            "Epoch 3, Batch 160/863, Loss: 0.6559\n",
            "Epoch 3, Batch 180/863, Loss: 0.5113\n",
            "Epoch 3, Batch 200/863, Loss: 0.5365\n",
            "Epoch 3, Batch 220/863, Loss: 0.5526\n",
            "Epoch 3, Batch 240/863, Loss: 0.5369\n",
            "Epoch 3, Batch 260/863, Loss: 0.5285\n",
            "Epoch 3, Batch 280/863, Loss: 0.6165\n",
            "Epoch 3, Batch 300/863, Loss: 0.4629\n",
            "Epoch 3, Batch 320/863, Loss: 0.4379\n",
            "Epoch 3, Batch 340/863, Loss: 0.6069\n",
            "Epoch 3, Batch 360/863, Loss: 0.4638\n",
            "Epoch 3, Batch 380/863, Loss: 0.5409\n",
            "Epoch 3, Batch 400/863, Loss: 0.6838\n",
            "Epoch 3, Batch 420/863, Loss: 0.4833\n",
            "Epoch 3, Batch 440/863, Loss: 0.4783\n",
            "Epoch 3, Batch 460/863, Loss: 0.6113\n",
            "Epoch 3, Batch 480/863, Loss: 0.4466\n",
            "Epoch 3, Batch 500/863, Loss: 0.4780\n",
            "Epoch 3, Batch 520/863, Loss: 0.7085\n",
            "Epoch 3, Batch 540/863, Loss: 0.4556\n",
            "Epoch 3, Batch 560/863, Loss: 0.5942\n",
            "Epoch 3, Batch 580/863, Loss: 0.4528\n",
            "Epoch 3, Batch 600/863, Loss: 0.5536\n",
            "Epoch 3, Batch 620/863, Loss: 0.5274\n",
            "Epoch 3, Batch 640/863, Loss: 0.6012\n",
            "Epoch 3, Batch 660/863, Loss: 0.5340\n",
            "Epoch 3, Batch 680/863, Loss: 0.5619\n",
            "Epoch 3, Batch 700/863, Loss: 0.5397\n",
            "Epoch 3, Batch 720/863, Loss: 0.4739\n",
            "Epoch 3, Batch 740/863, Loss: 0.6194\n",
            "Epoch 3, Batch 760/863, Loss: 0.6725\n",
            "Epoch 3, Batch 780/863, Loss: 0.5311\n",
            "Epoch 3, Batch 800/863, Loss: 0.6140\n",
            "Epoch 3, Batch 820/863, Loss: 0.6077\n",
            "Epoch 3, Batch 840/863, Loss: 0.5912\n",
            "Epoch 3, Batch 860/863, Loss: 0.6140\n",
            "Epoch 3 completed. Average Loss: 0.5486\n",
            "Epoch 4, Batch 0/863, Loss: 0.4106\n",
            "Epoch 4, Batch 20/863, Loss: 0.4720\n",
            "Epoch 4, Batch 40/863, Loss: 0.4807\n",
            "Epoch 4, Batch 60/863, Loss: 0.5366\n",
            "Epoch 4, Batch 80/863, Loss: 0.5834\n",
            "Epoch 4, Batch 100/863, Loss: 0.5107\n",
            "Epoch 4, Batch 120/863, Loss: 0.5344\n",
            "Epoch 4, Batch 140/863, Loss: 0.3719\n",
            "Epoch 4, Batch 160/863, Loss: 0.4554\n",
            "Epoch 4, Batch 180/863, Loss: 0.5083\n",
            "Epoch 4, Batch 200/863, Loss: 0.4840\n",
            "Epoch 4, Batch 220/863, Loss: 0.5519\n",
            "Epoch 4, Batch 240/863, Loss: 0.5950\n",
            "Epoch 4, Batch 260/863, Loss: 0.4015\n",
            "Epoch 4, Batch 280/863, Loss: 0.4435\n",
            "Epoch 4, Batch 300/863, Loss: 0.6029\n",
            "Epoch 4, Batch 320/863, Loss: 0.4125\n",
            "Epoch 4, Batch 340/863, Loss: 0.5203\n",
            "Epoch 4, Batch 360/863, Loss: 0.4430\n",
            "Epoch 4, Batch 380/863, Loss: 0.4598\n",
            "Epoch 4, Batch 400/863, Loss: 0.5181\n",
            "Epoch 4, Batch 420/863, Loss: 0.6243\n",
            "Epoch 4, Batch 440/863, Loss: 0.5523\n",
            "Epoch 4, Batch 460/863, Loss: 0.4420\n",
            "Epoch 4, Batch 480/863, Loss: 0.5132\n",
            "Epoch 4, Batch 500/863, Loss: 0.4548\n",
            "Epoch 4, Batch 520/863, Loss: 0.5483\n",
            "Epoch 4, Batch 540/863, Loss: 0.4141\n",
            "Epoch 4, Batch 560/863, Loss: 0.5952\n",
            "Epoch 4, Batch 580/863, Loss: 0.5813\n",
            "Epoch 4, Batch 600/863, Loss: 0.5022\n",
            "Epoch 4, Batch 620/863, Loss: 0.6120\n",
            "Epoch 4, Batch 640/863, Loss: 0.5644\n",
            "Epoch 4, Batch 660/863, Loss: 0.4586\n",
            "Epoch 4, Batch 680/863, Loss: 0.5396\n",
            "Epoch 4, Batch 700/863, Loss: 0.6462\n",
            "Epoch 4, Batch 720/863, Loss: 0.4657\n",
            "Epoch 4, Batch 740/863, Loss: 0.4631\n",
            "Epoch 4, Batch 760/863, Loss: 0.3962\n",
            "Epoch 4, Batch 780/863, Loss: 0.6068\n",
            "Epoch 4, Batch 800/863, Loss: 0.5469\n",
            "Epoch 4, Batch 820/863, Loss: 0.5038\n",
            "Epoch 4, Batch 840/863, Loss: 0.4511\n",
            "Epoch 4, Batch 860/863, Loss: 0.5055\n",
            "Epoch 4 completed. Average Loss: 0.5133\n",
            "Epoch 5, Batch 0/863, Loss: 0.4513\n",
            "Epoch 5, Batch 20/863, Loss: 0.4617\n",
            "Epoch 5, Batch 40/863, Loss: 0.4530\n",
            "Epoch 5, Batch 60/863, Loss: 0.4539\n",
            "Epoch 5, Batch 80/863, Loss: 0.3946\n",
            "Epoch 5, Batch 100/863, Loss: 0.6150\n",
            "Epoch 5, Batch 120/863, Loss: 0.4543\n",
            "Epoch 5, Batch 140/863, Loss: 0.4904\n",
            "Epoch 5, Batch 160/863, Loss: 0.3717\n",
            "Epoch 5, Batch 180/863, Loss: 0.4261\n",
            "Epoch 5, Batch 200/863, Loss: 0.5436\n",
            "Epoch 5, Batch 220/863, Loss: 0.7099\n",
            "Epoch 5, Batch 240/863, Loss: 0.4562\n",
            "Epoch 5, Batch 260/863, Loss: 0.3841\n",
            "Epoch 5, Batch 280/863, Loss: 0.4211\n",
            "Epoch 5, Batch 300/863, Loss: 0.5100\n",
            "Epoch 5, Batch 320/863, Loss: 0.3710\n",
            "Epoch 5, Batch 340/863, Loss: 0.5274\n",
            "Epoch 5, Batch 360/863, Loss: 0.4661\n",
            "Epoch 5, Batch 380/863, Loss: 0.6424\n",
            "Epoch 5, Batch 400/863, Loss: 0.4392\n",
            "Epoch 5, Batch 420/863, Loss: 0.5069\n",
            "Epoch 5, Batch 440/863, Loss: 0.3928\n",
            "Epoch 5, Batch 460/863, Loss: 0.4389\n",
            "Epoch 5, Batch 480/863, Loss: 0.4603\n",
            "Epoch 5, Batch 500/863, Loss: 0.5033\n",
            "Epoch 5, Batch 520/863, Loss: 0.5098\n",
            "Epoch 5, Batch 540/863, Loss: 0.4422\n",
            "Epoch 5, Batch 560/863, Loss: 0.4131\n",
            "Epoch 5, Batch 580/863, Loss: 0.4194\n",
            "Epoch 5, Batch 600/863, Loss: 0.5304\n",
            "Epoch 5, Batch 620/863, Loss: 0.4048\n",
            "Epoch 5, Batch 640/863, Loss: 0.4600\n",
            "Epoch 5, Batch 660/863, Loss: 0.4429\n",
            "Epoch 5, Batch 680/863, Loss: 0.4053\n",
            "Epoch 5, Batch 700/863, Loss: 0.4594\n",
            "Epoch 5, Batch 720/863, Loss: 0.3813\n",
            "Epoch 5, Batch 740/863, Loss: 0.4397\n",
            "Epoch 5, Batch 760/863, Loss: 0.5376\n",
            "Epoch 5, Batch 780/863, Loss: 0.4872\n",
            "Epoch 5, Batch 800/863, Loss: 0.4835\n",
            "Epoch 5, Batch 820/863, Loss: 0.5358\n",
            "Epoch 5, Batch 840/863, Loss: 0.6406\n",
            "Epoch 5, Batch 860/863, Loss: 0.3969\n",
            "Epoch 5 completed. Average Loss: 0.4876\n",
            "BPR training completed in 54.42 seconds\n",
            "\n",
            "=== IMPROVED EVALUATION ===\n",
            "\n",
            "BCE Loss:\n",
            "  Accuracy: 0.6379\n",
            "  Precision: 0.4298\n",
            "  Recall: 0.3989\n",
            "  F1-Score: 0.4138\n",
            "  AUC: 0.6149\n",
            "\n",
            "BPR Loss:\n",
            "  Accuracy: 0.5994\n",
            "  Precision: 0.3884\n",
            "  Recall: 0.4360\n",
            "  F1-Score: 0.4108\n",
            "  AUC: 0.5888\n",
            "\n",
            "=== RECOMMENDATIONS FOR USER AEXDJ6HWMOC6SRGLWALYYHWKXDIQ ===\n",
            "\n",
            "BCE Loss Recommendations:\n",
            "  1. Crompton Brio 1000-Watts Dry Iron with W... (Score: 0.8974)\n",
            "  2. KONVIO NEER 10 Inch Spun Filter (PP SPUN... (Score: 0.8898)\n",
            "  3. AVNISH Tap Water Purifier Filter Faucet ... (Score: 0.8887)\n",
            "\n",
            "BPR Loss Recommendations:\n",
            "  1. Luxor 5 Subject Single Ruled Notebook - ... (Score: 1.0000)\n",
            "  2. Butterfly Smart Mixer Grinder, 750W, 4 J... (Score: 1.0000)\n",
            "  3. HP 805 Black Original Ink Cartridge... (Score: 1.0000)\n",
            "\n",
            "==================================================\n",
            "FINAL MODEL COMPARISON\n",
            "==================================================\n",
            "\n",
            "BCE:\n",
            "  AUC: 0.6149 | F1: 0.4138 | Accuracy: 0.6379\n",
            "\n",
            "BPR:\n",
            "  AUC: 0.5888 | F1: 0.4108 | Accuracy: 0.5994\n",
            "\n",
            "=== TRAINING COMPLETED ===\n"
          ]
        }
      ]
    }
  ]
}